# Multimodal AI Assistant and Extended Reality (XR) Applications

## Project Overview

This project focuses on developing LLM-based AI agents and extended reality (XR) applications to enhance smart building control. Leveraging open-source models and Unity 3D, the project integrates the LLaVA vision language model, as well as open-source Text-to-Speech (TTS) and Speech-to-Text (STT) models. The application is designed for the Microsoft HoloLens 2, featuring AI-powered voice chat and image understanding capabilities.
<img src="/fig1.png" style="float: left; margin-right: 20px; max-width: 200px;">
### AI Voice Chat and Image Understanding

## Video Demo


[![Watch the demo video 1](https://img.youtube.com/vi/2esRfU4-7II/0.jpg)](https://www.youtube.com/watch?v=2esRfU4-7II)  
*Click on the image to view the video.*

[![Watch the demo video 2](https://img.youtube.com/vi/-Nxg_IkAl_c/0.jpg)](https://www.youtube.com/watch?v=-Nxg_IkAl_c)  
*Click on the image to view the video.*

### Key Features

- **LLM-Based AI Agents:** Utilizes advanced language models for intelligent interaction and control.
- **Extended Reality (XR) Integration:** Implements XR technologies with Unity 3D to create immersive smart building control applications.
- **AI Voice Chat:** Enables natural language communication with the smart building system.
- **Image Understanding:** Incorporates vision language models for understanding and interpreting visual data.

### Requirements
- Open-source Vision language model and Large Language Model (e.g., MiniCPM V, LLaMA 3)
- Generative AI inference tool. llama.cpp
- Unity 3D
- Microsoft Hololen 2
- Python 3.10
- Open-source Text-to-Speech (TTS) model, Whisper
- Open-source Speech-to-Text (STT) model, Piper


## Detailed setup guide
Coming soon.....

## License
This project is licensed under the MIT License.


