# Multimodal AI Assistant and Extended Reality (XR) Applications

## Project Description

This project explores the integration of artificial intelligence (AI) and extended reality (XR) to enable advanced smart building control in immersive environments. By combining large language models (LLMs), digital twins, and extended reality platforms, this research aims to create an innovative solution for remote facility management and urban infrastructure monitoring.

The developed system deploys an LLM-based AI assistant and a digital building twin into an XR environment using Microsoft HoloLens 2. Users can visualize real-time building performance data and interact with the AI assistant through voice commands to control building facilities. This setup enhances the ability of facility managers and occupants to interact with and control smart buildings remotely. The approach also holds potential for scaling to multiple buildings or urban infrastructure, enabling immersive, real-time monitoring and management for smart city applications.

To achieve this, the project leverages open-source AI models and XR technologies:
- **LLM-Based AI Agents**: Advanced language models facilitate intelligent and natural interaction with building systems.
- **Extended Reality (XR) Integration**: Unity 3D is used to create immersive applications, including real-time building visualization and BIM model manipulation.
- **AI-Powered Voice Interaction**: Open-source Text-to-Speech (TTS) and Speech-to-Text (STT) models enable natural language communication for building system control.
- **Vision-Language Models**: Integration of the LLaVA vision language model provides image understanding capabilities, allowing the AI to interpret and respond to visual data.

This work demonstrates the potential of combining generative AI with XR technologies for practical applications in smart building management. It paves the way for immersive, AI-enhanced environments that can transform how we interact with and manage built infrastructure, contributing to the vision of responsive and interconnected smart cities.


<img src="/fig1.png" style="float: left; margin-right: 20px; max-width: 200px;">
### AI Voice Chat and Image Understanding

## Video Demo


<table>
  <tr>
    <td style="text-align: center;">
      <a href="https://www.youtube.com/watch?v=2esRfU4-7II" target="_blank">
        <img src="https://img.youtube.com/vi/2esRfU4-7II/0.jpg" alt="Demo Video 1" style="width: 300px;">
      </a>
      <p>*Click on the image to view Demo Video 1.*</p>
    </td>
    <td style="text-align: center;">
      <a href="https://www.youtube.com/watch?v=-Nxg_IkAl_c" target="_blank">
        <img src="https://img.youtube.com/vi/-Nxg_IkAl_c/0.jpg" alt="Demo Video 2" style="width: 300px;">
      </a>
      <p>*Click on the image to view Demo Video 2.*</p>
    </td>
  </tr>
</table>

### Key Features

- **LLM-Based AI Agents:** Utilizes advanced language models for intelligent interaction and control.
- **Extended Reality (XR) Integration:** Implements XR technologies with Unity 3D to create immersive smart building control applications as well as BIM model manipulation.
- **AI Voice Chat:** Enables natural language communication with the smart building system.
- **Image Understanding:** Incorporates vision language models for understanding and interpreting visual data.

### Requirements
- Open-source Vision language model and Large Language Model (e.g., MiniCPM V, LLaMA 3)
- Generative AI inference tool. llama.cpp
- Unity 3D
- Microsoft Hololen 2
- Python 3.10
- Open-source Text-to-Speech (TTS) model, Whisper
- Open-source Speech-to-Text (STT) model, Piper


## Detailed setup guide
Coming soon.....

## License
This project is licensed under the MIT License.


